"""
GFKæ•°æ®å¤„ç†ç®¡é“

è¿™æ˜¯æ ¸å¿ƒPipelineç±»ï¼Œåè°ƒæ‰€æœ‰æ•°æ®å¤„ç†æ­¥éª¤ï¼Œæä¾›ç»Ÿä¸€çš„ETLæ¥å£ã€‚
"""

import pandas as pd
import os
from datetime import datetime
from typing import Dict, List, Any, Optional, Union

from .config import ConfigManager
from .core import DataLoader, DataCleaner, DataTransformer, DataValidator, DataExporter
from .utils import print_dataframe_summary, create_progress_logger


class GFKDataPipeline:
    """GFKæ•°æ®å¤„ç†ç®¡é“ä¸»ç±»"""
    
    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†ç®¡é“
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            
        Raises:
            FileNotFoundError: å½“é…ç½®æ–‡ä»¶ä¸å­˜åœ¨æ—¶
            Exception: å½“åˆå§‹åŒ–å¤±è´¥æ—¶
        """
        try:
            print(f"ğŸš€ åˆå§‹åŒ–GFKæ•°æ®å¤„ç†ç®¡é“")
            print(f"ğŸ“‹ é…ç½®æ–‡ä»¶: {config_path}")
            
            # åŠ è½½é…ç½®
            self.config = ConfigManager(config_path)
            
            # åˆå§‹åŒ–å„ä¸ªå¤„ç†æ¨¡å—
            self._initialize_modules()
            
            # åˆå§‹åŒ–ç»“æœå­˜å‚¨
            self.results = {
                'pipeline_start_time': datetime.now(),
                'config_path': config_path,
                'region': self.config.get('data_sources.region', 'UNKNOWN'),
                'processing_stages': {}
            }
            
            print(f"âœ… ç®¡é“åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ ç®¡é“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def _initialize_modules(self) -> None:
        """åˆå§‹åŒ–æ‰€æœ‰å¤„ç†æ¨¡å—"""
        
        # åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        input_dir = self.config.get('data_sources.input_directory', '.')
        self.loader = DataLoader(input_dir)
        
        # åˆå§‹åŒ–æ•°æ®æ¸…æ´—å™¨
        cleaning_config = self.config.get('processing.cleaning', {})
        self.cleaner = DataCleaner(cleaning_config)
        
        # åˆå§‹åŒ–æ•°æ®è½¬æ¢å™¨
        transform_config = self.config.get('processing', {})
        self.transformer = DataTransformer(transform_config)
        
        # åˆå§‹åŒ–æ•°æ®éªŒè¯å™¨
        validation_config = self.config.get('validation', {})
        self.validator = DataValidator(validation_config)
        
        # åˆå§‹åŒ–æ•°æ®å¯¼å‡ºå™¨
        output_config = self.config.get('output', {})
        output_config['output_directory'] = self.config.get('data_sources.output_directory', './data/processed')
        self.exporter = DataExporter(output_config)
        
        print(f"ğŸ“¦ æ‰€æœ‰å¤„ç†æ¨¡å—åˆå§‹åŒ–å®Œæˆ")
    
    def run(self, export_data: bool = True, 
            export_validation: bool = True) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†ç®¡é“
        
        Args:
            export_data: æ˜¯å¦å¯¼å‡ºå¤„ç†åçš„æ•°æ®
            export_validation: æ˜¯å¦å¯¼å‡ºéªŒè¯æŠ¥å‘Š
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            print(f"\n{'='*80}")
            print(f" ğŸš€ å¼€å§‹æ‰§è¡ŒGFKæ•°æ®å¤„ç†ç®¡é“")
            print(f" ğŸ“… å¼€å§‹æ—¶é—´: {self.results['pipeline_start_time'].strftime('%Y-%m-%d %H:%M:%S')}")
            print(f" ğŸŒ å¤„ç†åŒºåŸŸ: {self.results['region']}")
            print(f"{'='*80}")
            
            # åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
            total_steps = 5  # åŠ è½½ã€æ¸…æ´—ã€è½¬æ¢ã€éªŒè¯ã€å¯¼å‡º
            progress_logger = create_progress_logger(total_steps)
            
            # ç¬¬1æ­¥ï¼šæ•°æ®åŠ è½½
            progress_logger(1, "æ•°æ®åŠ è½½")
            raw_data = self._load_data()
            if not raw_data:
                raise Exception("æ•°æ®åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å¤„ç†")
            
            # ç¬¬2æ­¥ï¼šæ•°æ®æ¸…æ´—
            progress_logger(2, "æ•°æ®æ¸…æ´—")
            cleaned_data = self._clean_data(raw_data)
            
            # ç¬¬3æ­¥ï¼šæ•°æ®è½¬æ¢
            progress_logger(3, "æ•°æ®è½¬æ¢")
            transformed_data = self._transform_data(cleaned_data)
            
            # ç¬¬4æ­¥ï¼šæ•°æ®éªŒè¯
            progress_logger(4, "æ•°æ®éªŒè¯")
            validation_results = self._validate_data(transformed_data)
            
            # ç¬¬5æ­¥ï¼šæ•°æ®å¯¼å‡º
            progress_logger(5, "æ•°æ®å¯¼å‡º")
            if export_data or export_validation:
                export_results = self._export_data(transformed_data, validation_results,
                                                 export_data, export_validation)
                self.results['export_results'] = export_results
            
            # å®Œæˆå¤„ç†
            self.results['pipeline_end_time'] = datetime.now()
            self.results['pipeline_duration'] = (
                self.results['pipeline_end_time'] - self.results['pipeline_start_time']
            ).total_seconds()
            
            self.results['final_data'] = transformed_data
            self.results['validation_results'] = validation_results
            self.results['success'] = True
            
            self._print_final_summary()
            
            return self.results
            
        except Exception as e:
            print(f"\nâŒ ç®¡é“æ‰§è¡Œå¤±è´¥: {str(e)}")
            self.results['success'] = False
            self.results['error'] = str(e)
            self.results['pipeline_end_time'] = datetime.now()
            return self.results
    
    def _load_data(self) -> Optional[Dict[str, pd.DataFrame]]:
        """
        æ•°æ®åŠ è½½æ­¥éª¤
        
        Returns:
            åŠ è½½çš„æ•°æ®å­—å…¸
        """
        try:
            print(f"\nğŸ“¥ ç¬¬1æ­¥ï¼šæ•°æ®åŠ è½½")
            
            # æ ¹æ®é…ç½®ç±»å‹é€‰æ‹©åŠ è½½æ–¹å¼
            if self.config.get('data_sources.countries'):
                # æ¬§æ´²å¤šå›½æ•°æ®
                countries_config = self.config.get_countries()
                data_dict = self.loader.load_country_files(countries_config)
                
            elif self.config.get('data_sources.spain_files'):
                # è¥¿ç­ç‰™å¤šæ–‡ä»¶æ•°æ®
                spain_config = self.config.get_spain_files()
                data_dict = self.loader.load_spain_files(spain_config)
                
            else:
                print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®æºé…ç½®")
                return None
            
            if not data_dict:
                print("âŒ æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®æ–‡ä»¶")
                return None
            
            # è®°å½•åŠ è½½ç»“æœ
            load_summary = {
                'files_loaded': len(data_dict),
                'total_rows': sum(len(df) for df in data_dict.values()),
                'data_sources': list(data_dict.keys())
            }
            
            self.results['processing_stages']['data_loading'] = load_summary
            print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {load_summary['files_loaded']} ä¸ªæ–‡ä»¶ï¼Œå…± {load_summary['total_rows']:,} è¡Œ")
            
            return data_dict
            
        except Exception as e:
            print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def _clean_data(self, raw_data: Dict[str, pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """
        æ•°æ®æ¸…æ´—æ­¥éª¤
        
        Args:
            raw_data: åŸå§‹æ•°æ®å­—å…¸
            
        Returns:
            æ¸…æ´—åçš„æ•°æ®å­—å…¸
        """
        print(f"\nğŸ§¹ ç¬¬2æ­¥ï¼šæ•°æ®æ¸…æ´—")
        
        cleaned_data = {}
        cleaning_summary = {
            'files_cleaned': 0,
            'total_rows_before': 0,
            'total_rows_after': 0,
            'rows_removed': 0
        }
        
        for name, df in raw_data.items():
            if df is None or df.empty:
                continue
            
            rows_before = len(df)
            cleaned_df = self.cleaner.clean_dataframe(df, f"{name}æ•°æ®")
            rows_after = len(cleaned_df)
            
            if cleaned_df is not None and not cleaned_df.empty:
                cleaned_data[name] = cleaned_df
                cleaning_summary['files_cleaned'] += 1
                cleaning_summary['total_rows_before'] += rows_before
                cleaning_summary['total_rows_after'] += rows_after
                cleaning_summary['rows_removed'] += (rows_before - rows_after)
        
        self.results['processing_stages']['data_cleaning'] = cleaning_summary
        print(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆ: {cleaning_summary['total_rows_before']:,} â†’ {cleaning_summary['total_rows_after']:,} è¡Œ")
        
        return cleaned_data
    
    def _transform_data(self, cleaned_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """
        æ•°æ®è½¬æ¢æ­¥éª¤
        
        Args:
            cleaned_data: æ¸…æ´—åçš„æ•°æ®å­—å…¸
            
        Returns:
            è½¬æ¢å¹¶åˆå¹¶åçš„DataFrame
        """
        print(f"\nğŸ”„ ç¬¬3æ­¥ï¼šæ•°æ®è½¬æ¢")
        
        transformed_data_list = []
        transform_summary = {
            'files_transformed': 0,
            'total_rows_before_transform': 0,
            'total_rows_after_transform': 0
        }
        
        # è½¬æ¢æ¯ä¸ªæ•°æ®é›†
        for name, df in cleaned_data.items():
            if df is None or df.empty:
                continue
            
            rows_before = len(df)
            transformed_df = self.transformer.transform_dataframe(df, f"{name}æ•°æ®")
            
            if transformed_df is not None and not transformed_df.empty:
                transformed_data_list.append(transformed_df)
                transform_summary['files_transformed'] += 1
                transform_summary['total_rows_before_transform'] += rows_before
                transform_summary['total_rows_after_transform'] += len(transformed_df)
        
        # åˆå¹¶æ‰€æœ‰è½¬æ¢åçš„æ•°æ®
        if transformed_data_list:
            print(f"\nğŸ”— åˆå¹¶ {len(transformed_data_list)} ä¸ªæ•°æ®é›†")
            combined_df = pd.concat(transformed_data_list, ignore_index=True)
            print_dataframe_summary(combined_df, "åˆå¹¶åæ•°æ®")
            
            # æ‰§è¡Œé€è§†æ“ä½œ
            pivot_config = self.config.get_pivot_config()
            if pivot_config:
                final_df = self.transformer.pivot_by_facts(combined_df, pivot_config)
            else:
                final_df = combined_df
            
            transform_summary['final_rows'] = len(final_df)
            transform_summary['final_columns'] = len(final_df.columns)
            
        else:
            print("âŒ æ²¡æœ‰å¯åˆå¹¶çš„è½¬æ¢æ•°æ®")
            final_df = pd.DataFrame()
        
        self.results['processing_stages']['data_transformation'] = transform_summary
        print(f"âœ… æ•°æ®è½¬æ¢å®Œæˆ: æœ€ç»ˆ {len(final_df):,} è¡Œ Ã— {len(final_df.columns)} åˆ—")
        
        return final_df
    
    def _validate_data(self, transformed_data: pd.DataFrame) -> Dict[str, Any]:
        """
        æ•°æ®éªŒè¯æ­¥éª¤
        
        Args:
            transformed_data: è½¬æ¢åçš„æ•°æ®
            
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        print(f"\nâœ… ç¬¬4æ­¥ï¼šæ•°æ®éªŒè¯")
        
        if transformed_data is None or transformed_data.empty:
            print("âš ï¸  æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡éªŒè¯")
            return {'passed': False, 'reason': 'empty_data'}
        
        # æ‰§è¡ŒéªŒè¯
        validation_results = self.validator.validate_dataframe(
            transformed_data, 
            f"{self.results['region']}æœ€ç»ˆæ•°æ®"
        )
        
        self.results['processing_stages']['data_validation'] = {
            'validation_passed': validation_results.get('passed', False),
            'issues_found': len(validation_results.get('issues', [])),
            'consistency_rate': validation_results.get('consistency_check', {}).get('consistency_rate', 0)
        }
        
        return validation_results
    
    def _export_data(self, transformed_data: pd.DataFrame, 
                    validation_results: Dict[str, Any],
                    export_data: bool = True,
                    export_validation: bool = True) -> Dict[str, str]:
        """
        æ•°æ®å¯¼å‡ºæ­¥éª¤
        
        Args:
            transformed_data: è¦å¯¼å‡ºçš„æ•°æ®
            validation_results: éªŒè¯ç»“æœ
            export_data: æ˜¯å¦å¯¼å‡ºæ•°æ®
            export_validation: æ˜¯å¦å¯¼å‡ºéªŒè¯æŠ¥å‘Š
            
        Returns:
            å¯¼å‡ºæ–‡ä»¶è·¯å¾„å­—å…¸
        """
        print(f"\nğŸ’¾ ç¬¬5æ­¥ï¼šæ•°æ®å¯¼å‡º")
        
        export_results = {}
        
        if export_data and transformed_data is not None and not transformed_data.empty:
            # å¯¼å‡ºæ•°æ®
            if export_validation and self.config.get('output.save_validation_report', False):
                # åŒæ—¶å¯¼å‡ºæ•°æ®å’ŒéªŒè¯æŠ¥å‘Š
                export_paths = self.exporter.export_with_validation_report(
                    transformed_data, validation_results, self.results['region']
                )
                export_results.update(export_paths)
            else:
                # åªå¯¼å‡ºæ•°æ®
                data_path = self.exporter.export_dataframe(
                    transformed_data, region=self.results['region']
                )
                if data_path:
                    export_results['data'] = data_path
        
        elif export_validation:
            # åªå¯¼å‡ºéªŒè¯æŠ¥å‘Š
            report_path = self.exporter._export_validation_report(
                validation_results, self.results['region']
            )
            if report_path:
                export_results['validation_report'] = report_path
        
        self.results['processing_stages']['data_export'] = {
            'files_exported': len(export_results),
            'export_paths': export_results
        }
        
        print(f"âœ… æ•°æ®å¯¼å‡ºå®Œæˆ: {len(export_results)} ä¸ªæ–‡ä»¶")
        for file_type, path in export_results.items():
            print(f"   {file_type}: {path}")
        
        return export_results
    
    def _print_final_summary(self) -> None:
        """æ‰“å°æœ€ç»ˆå¤„ç†æ€»ç»“"""
        print(f"\n{'='*80}")
        print(f" ğŸ‰ GFKæ•°æ®å¤„ç†ç®¡é“æ‰§è¡Œå®Œæˆ")
        print(f"{'='*80}")
        
        duration = self.results.get('pipeline_duration', 0)
        print(f"â±ï¸  æ€»è€—æ—¶: {duration:.2f} ç§’")
        print(f"ğŸ“Š å¤„ç†çŠ¶æ€: {'âœ… æˆåŠŸ' if self.results.get('success', False) else 'âŒ å¤±è´¥'}")
        
        # å„é˜¶æ®µæ€»ç»“
        for stage, summary in self.results.get('processing_stages', {}).items():
            print(f"\nğŸ“‹ {stage}:")
            for key, value in summary.items():
                if isinstance(value, (int, float)):
                    if 'rows' in key.lower():
                        print(f"   {key}: {value:,}")
                    else:
                        print(f"   {key}: {value}")
                else:
                    print(f"   {key}: {value}")
        
        # æœ€ç»ˆæ•°æ®ç»Ÿè®¡
        final_data = self.results.get('final_data')
        if final_data is not None and not final_data.empty:
            print(f"\nğŸ“ˆ æœ€ç»ˆæ•°æ®:")
            print(f"   è¡Œæ•°: {len(final_data):,}")
            print(f"   åˆ—æ•°: {len(final_data.columns)}")
            
            # æ˜¾ç¤ºåˆ—ä¿¡æ¯
            if len(final_data.columns) <= 15:
                print(f"   åˆ—å: {', '.join(final_data.columns)}")
            else:
                print(f"   å‰5åˆ—: {', '.join(final_data.columns[:5])}...")
        
        print(f"\n{'='*80}")
    
    def get_summary_report(self) -> str:
        """
        è·å–å¤„ç†æ€»ç»“æŠ¥å‘Š
        
        Returns:
            æ ¼å¼åŒ–çš„æ€»ç»“æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        lines = []
        lines.append("GFKæ•°æ®å¤„ç†ç®¡é“æ‰§è¡ŒæŠ¥å‘Š")
        lines.append("="*50)
        lines.append(f"é…ç½®æ–‡ä»¶: {self.results.get('config_path', 'Unknown')}")
        lines.append(f"å¤„ç†åŒºåŸŸ: {self.results.get('region', 'Unknown')}")
        lines.append(f"å¼€å§‹æ—¶é—´: {self.results.get('pipeline_start_time', 'Unknown')}")
        lines.append(f"ç»“æŸæ—¶é—´: {self.results.get('pipeline_end_time', 'Unknown')}")
        lines.append(f"æ€»è€—æ—¶: {self.results.get('pipeline_duration', 0):.2f} ç§’")
        lines.append(f"æ‰§è¡ŒçŠ¶æ€: {'æˆåŠŸ' if self.results.get('success', False) else 'å¤±è´¥'}")
        lines.append("")
        
        # å„é˜¶æ®µè¯¦æƒ…
        for stage, summary in self.results.get('processing_stages', {}).items():
            lines.append(f"ã€{stage}ã€‘")
            for key, value in summary.items():
                lines.append(f"  {key}: {value}")
            lines.append("")
        
        return "\n".join(lines)
    
    def export_summary_report(self, filename: Optional[str] = None) -> str:
        """
        å¯¼å‡ºå¤„ç†æ€»ç»“æŠ¥å‘Š
        
        Args:
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å¯¼å‡ºæ–‡ä»¶è·¯å¾„
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"GFK_{self.results['region']}_PIPELINE_SUMMARY_{timestamp}.txt"
        
        summary_content = self.get_summary_report()
        return self.exporter.create_summary_export({'pipeline_summary': summary_content}, filename)
    
    def __str__(self) -> str:
        """è¿”å›ç®¡é“çš„å­—ç¬¦ä¸²è¡¨ç¤º"""
        return f"GFKDataPipeline(region='{self.results.get('region', 'Unknown')}')"
    
    def __repr__(self) -> str:
        """è¿”å›ç®¡é“çš„è¯¦ç»†è¡¨ç¤º"""
        return f"GFKDataPipeline(config='{self.results.get('config_path', 'Unknown')}', region='{self.results.get('region', 'Unknown')}')"